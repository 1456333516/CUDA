#include <thrust/execution_policy.h>  // 包含Thrust执行策略头文件
#include <thrust/scan.h>              // 包含Thrust扫描操作函数
#include <stdio.h>                    // 包含标准输入输出库

/*
 * ==================== 程序概述 ====================
 * 这是一个混合使用Thrust库和原始CUDA指针的前缀扫描示例
 * 
 * 与之前的vector版本不同，这里直接使用原始指针管理内存
 * 展示了如何在传统CUDA编程风格中集成Thrust库的功能
 * 
 * 程序流程：
 * 1. 在主机端分配内存并初始化数据
 * 2. 将数据传输到GPU显存
 * 3. 使用Thrust执行GPU上的前缀扫描
 * 4. 将结果传回主机并打印
 * 5. 清理所有分配的内存资源
 */

int main()
{
    /*
     * ==================== 数据规模定义 ====================
     */
    int N = 10;  // 定义处理的数据元素个数为10
    
    /*
     * ==================== GPU端指针声明 ====================
     * 声明指向GPU显存的原始指针
     * 这种方式给了程序员更大的控制权，但也需要手动管理内存
     */
    int *x, *y;  // x: 输入数据指针, y: 输出结果指针
    
    /*
     * ==================== GPU显存分配 ====================
     * 使用cudaMalloc为GPU显存分配空间
     * 
     * 参数说明：
     * - (void **)&x: 二级指针，cudaMalloc需要修改指针本身的值
     * - sizeof(int) * N: 分配的字节数（整数个数 × 每个整数的字节数）
     * 
     * 内存布局：
     * x指向的显存区域: [int][int][int]...[int] (共N个int)
     * y指向的显存区域: [int][int][int]...[int] (共N个int)
     */
    cudaMalloc((void **)&x, sizeof(int) * N);  // 为输入数据分配GPU显存
    cudaMalloc((void **)&y, sizeof(int) * N);  // 为输出结果分配GPU显存
    
    /*
     * ==================== 主机端内存分配 ====================
     * 在主机(CPU)端分配内存用于数据准备和结果接收
     */
    int *h_x = (int *)malloc(sizeof(int) * N);  // 主机端输入数据缓冲区
    // 注意：这里没有初始化h_y，因为在后面会从GPU复制数据过来
    
    /*
     * ==================== 主机端数据初始化 ====================
     * 在CPU内存中准备输入数据 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
     */
    for (int i = 0; i < N; i++)      // 循环遍历所有元素
    {
        h_x[i] = i + 1;              // 第i个元素赋值为i+1
                                     // 结果：h_x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    }
    
    /*
     * ==================== 数据传输：主机→设备 ====================
     * 将主机端准备好的数据复制到GPU显存中
     * 
     * cudaMemcpy参数详解：
     * - x: 目标地址（GPU显存）
     * - h_x: 源地址（主机内存）
     * - sizeof(int) * N: 传输的数据大小
     * - cudaMemcpyHostToDevice: 传输方向（主机到设备）
     */
    cudaMemcpy(x, h_x, sizeof(int) * N, cudaMemcpyHostToDevice);
    // 现在GPU显存中的x包含了 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    
    /*
     * ==================== 核心计算：Thrust前缀扫描 ====================
     * 使用Thrust库在GPU上执行前缀扫描操作
     * 
     * 函数签名：thrust::inclusive_scan(执行策略, 起始迭代器, 结束迭代器, 输出迭代器)
     * 
     * 参数详解：
     * - thrust::device: 执行策略，指定在GPU设备上执行
     * - x: 输入数据的起始地址（迭代器概念）
     * - x + N: 输入数据的结束地址（迭代器概念）
     * - y: 输出结果的起始地址
     * 
     * 执行过程（以输入[1,2,3,4]为例）：
     * y[0] = x[0] = 1
     * y[1] = y[0] + x[1] = 1 + 2 = 3
     * y[2] = y[1] + x[2] = 3 + 3 = 6
     * y[3] = y[2] + x[3] = 6 + 4 = 10
     * 
     * 关键优势：
     * 1. 自动并行化：Thrust内部使用优化的CUDA实现
     * 2. 类型安全：编译时检查迭代器类型匹配
     * 3. 高性能：利用GPU大规模并行计算能力
     */
    thrust::inclusive_scan(thrust::device, x, x + N, y);
    
    /*
     * ==================== 结果接收：设备→主机 ====================
     * 为主机端分配接收结果的内存缓冲区
     */
    int *h_y = (int *)malloc(sizeof(int) * N);  // 主机端输出数据缓冲区
    
    /*
     * 将GPU计算结果复制回主机内存
     * 
     * cudaMemcpy参数：
     * - h_y: 目标地址（主机内存）
     * - y: 源地址（GPU显存）
     * - sizeof(int) * N: 传输的数据大小
     * - cudaMemcpyDeviceToHost: 传输方向（设备到主机）
     */
    cudaMemcpy(h_y, y, sizeof(int) * N, cudaMemcpyDeviceToHost);
    // 现在h_y包含了GPU计算的结果
    
    /*
     * ==================== 结果输出 ====================
     * 在主机端打印计算结果
     */
    printf("Scan results: ");
    for(int i = 0; i < N; i++)       // 遍历所有结果元素
    {
        printf("%d ", h_y[i]);       // 打印每个元素的值
    }
    printf("\n");
    
    /*
     * ==================== 内存清理 ====================
     * 释放所有分配的内存资源，避免内存泄漏
     * 
     * 释放顺序很重要：
     * 1. 先释放GPU显存（cudaFree）
     * 2. 再释放主机内存（free）
     */
    cudaFree(x);     // 释放GPU端输入数据内存
    cudaFree(y);     // 释放GPU端输出数据内存
    free(h_x);       // 释放主机端输入数据内存
    free(h_y);       // 释放主机端输出数据内存
    
    /*
     * ==================== 程序结束 ====================
     * 返回0表示程序正常执行完毕
     */
    return 0;
}

/*
 * ==================== 预期输出 ====================
 * Scan results: 1 3 6 10 15 21 28 36 45 55 
 * 
 * 计算验证：
 * 1 = 1
 * 3 = 1 + 2
 * 6 = 1 + 2 + 3
 * 10 = 1 + 2 + 3 + 4
 * 15 = 1 + 2 + 3 + 4 + 5
 * 21 = 1 + 2 + 3 + 4 + 5 + 6
 * 28 = 1 + 2 + 3 + 4 + 5 + 6 + 7
 * 36 = 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8
 * 45 = 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9
 * 55 = 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10
 */

/*
 * ==================== 技术要点总结 ====================
 * 
 * 1. 内存管理模式对比：
 *    - device_vector版本：自动管理内存，代码简洁
 *    - 原始指针版本：手动管理内存，控制精细
 * 
 * 2. Thrust与传统CUDA结合：
 *    - 可以在传统CUDA代码中使用Thrust的高性能算法
 *    - 保持了对内存的完全控制权
 *    - 适合需要精细内存管理的复杂应用
 * 
 * 3. 执行策略：
 *    - thrust::device: 在GPU上执行
 *    - thrust::host: 在CPU上执行
 *    - 可以根据需求灵活选择执行平台
 * 
 * 4. 性能考虑：
 *    - 数据传输是瓶颈：尽量减少主机-设备间的数据传输
 *    - 批量处理：一次传输大量数据比分多次传输效率高
 *    - 内存对齐：合理规划内存布局可提升访问效率
 */